<h1 id="overview-of-marss-residuals">Overview of MARSS residuals</h1>
<p>This report discusses the computation of the variance of the conditional model and state residuals for MARSS models of the form: <span class="math display">\[\begin{equation}\label{eq:residsMARSS}
\begin{gathered}
\xx_t = \BB_t\xx_{t-1} + \uu_t + \ww_t, \text{ where } \WW_t \sim \MVN(0,\QQ_t)\\
\yy_t = \ZZ_t\xx_t + \aa_t + \vv_t, \text{ where } \VV_t \sim \MVN(0,\RR_t)\\
\XX_0 \sim \MVN(\xixi,\LAM) \text{ or } \xx_0 = \xixi .
\end{gathered}
\end{equation}\]</span> The state and model residuals are respectively <span class="math display">\[\begin{equation}\label{eq:resids}
\begin{gathered}
\ww_t = \xx_t - \BB_t\xx_{t-1} - \uu_t\\
\vv_t = \yy_t - \ZZ_t\xx_t - \aa_t .
\end{gathered}
\end{equation}\]</span> The model (and state) residuals are a random variables since <span class="math inline">\(\yy_t\)</span> and <span class="math inline">\(\xx_t\)</span> are drawn from the joint multivariate distribution of <span class="math inline">\(\YY_t\)</span> and <span class="math inline">\(\XX_t\)</span> defined by the MARSS equations (Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>). The unconditional variance of the model residuals is <span class="math display">\[\begin{equation}\label{eq:unconditiondistofVt}
\var_{XY}[\VV_t] = \var_{XY}[\YY_t - (\ZZ_t \XX_t + \aa_t)] = \RR_t\\
\end{equation}\]</span> based on the distribution of <span class="math inline">\(\VV_t\)</span> in Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>. <span class="math inline">\(\var_{XY}\)</span> indicates that the integration is over the joint unconditional distribution of <span class="math inline">\(\XX\)</span> and <span class="math inline">\(\YY\)</span>.</p>
<p>Once we have data, <span class="math inline">\(\RR_t\)</span> is not the variance-covariance matrix of our model residuals because our residuals are now conditioned on a set of observed data. There are two types of conditional model residuals used in MARSS analyses: innovations and smoothations. Innovations are the model residuals at time <span class="math inline">\(t\)</span> using the expected value of <span class="math inline">\(\XX_t\)</span> conditioned on the data from 1 to <span class="math inline">\(t-1\)</span>. Smoothations are the model residuals using the expected value of <span class="math inline">\(\XX_t\)</span> conditioned on all the data, <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span>. Smoothations are used in computing standardized residuals for outlier and structural break detection <span class="citation" data-cites="HarveyKoopman1992 Harveyetal1998 deJongPenzer1998 CommandeurKoopman2007">(<a href="#ref-CommandeurKoopman2007" role="doc-biblioref">Commandeur &amp; Koopman, 2007</a>; <a href="#ref-Harveyetal1998" role="doc-biblioref">Harvey et al., 1998</a>; <a href="#ref-HarveyKoopman1992" role="doc-biblioref">Harvey &amp; Koopman, 1992</a>; <a href="#ref-deJongPenzer1998" role="doc-biblioref">Jong &amp; Penzer, 1998</a>)</span>.</p>
<p>It should be noted that all the calculations discussed here are conditioned on the MARSS parameters: <span class="math inline">\(\BB\)</span>, <span class="math inline">\(\QQ\)</span>, <span class="math inline">\(\UU\)</span>, <span class="math inline">\(\RR\)</span>, <span class="math inline">\(\ZZ\)</span> and <span class="math inline">\(\AA\)</span>. These are treated as known. This is different than standard discussions of residual distributions for linear models where the uncertainty in the model parameters enters into the calculations (as it enters into the calculation of the influence of <span class="math inline">\(\yy\)</span> on the expected (or fitted) value of <span class="math inline">\(\yy\)</span>). In the calculations in this report, <span class="math inline">\(\yy\)</span> does not affect the estimates of the parameters (which are fixed, perhaps at estimated values) but does affect the expected value of <span class="math inline">\(\YY_t\)</span> by affecting the estimate of the expected value and variance of <span class="math inline">\(\XX_t\)</span>.</p>
<h1 id="sec:smoothations">Distribution of MARSS smoothation residuals</h1>
<div style="display:none">
<span class="math inline">\(\nextSection\)</span>
</div>
<p>This section discusses computation of the variance of the model and state residuals conditioned on all the data from <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span>. These MARSS residuals are often used for outlier detection and shock detection, and in this case you only need the distribution of the model residuals for the observed values. However if you wanted to do a leave-one-out cross-validation, you would need to know the distribution of the residuals for data points you left out (treated as unobserved). The equations in this report give you the former and the latter, while the algorithm by <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> gives only the former. These equations for residuals for “left-out” data are different that other (typical) discussions of state-space cross-validation <span class="citation" data-cites="deJong1988">(<a href="#ref-deJong1988" role="doc-biblioref">Jong, 1988</a>)</span> in that they are conditioned on all the data (smoothations residuals) rather than conditioned on data up to <span class="math inline">\(t-1\)</span> (innovations residuals).</p>
<h2 id="notation-and-relations">Notation and relations</h2>
<p>Throughout, I follow the convention that capital letters are random variables and small letters are a realization from the random variable. This only applies to random variables; parameters are not random variables. Parameters are shown in Roman font while while random variables are bold slanted font. Parameters written as capital letters are matrices, while parameters written in small letters are strictly column matrices.</p>
<p>In this report, the distribution over which the integration is done in an expectation or variance is given by the subscript; e.g., <span class="math inline">\(\E_A[f(A)]\)</span> indicates an unconditional expectation over the distribution of <span class="math inline">\(A\)</span> without conditioning on another random variable while <span class="math inline">\(\E_{A|b}[f(A)|b]\)</span> would indicate an expectation over the distribution of <span class="math inline">\(A\)</span> conditioned on <span class="math inline">\(B=b\)</span>; presumably <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent otherwise <span class="math inline">\(B=b\)</span> would have no effect on <span class="math inline">\(A\)</span>. <span class="math inline">\(\E_{A|b}[f(A)|b]\)</span> is a fixed value, not random. It is the expected value when <span class="math inline">\(B=b\)</span>. In contrast, <span class="math inline">\(\E_{A|B}[f(A)|B]\)</span> denotes the random variable over all the possible <span class="math inline">\(\E_{A|b}[f(A)|b]\)</span> given all the possible <span class="math inline">\(b\)</span> values that <span class="math inline">\(B\)</span> might take. The variance of <span class="math inline">\(\E_{A|B}[f(A)|B]\)</span> is the variance of this random variable. The variance of <span class="math inline">\(\E_{A|b}[f(A)|b]\)</span> in contrast is 0 since it is a fixed value. We will often be working with the random variables, <span class="math inline">\(\E_{A|B}[f(A)|B]\)</span> or <span class="math inline">\(\var_{A|B}[f(A)|B]\)</span>, inside an expectation or variance: such as <span class="math inline">\(\var_B[\E_{A|B}[f(A)|B]]\)</span>.</p>
<h3 id="law-of-total-variance">Law of total variance</h3>
<p>The law of total variance can be written <span class="math display">\[\begin{equation}\label{eq:lawoftotvar}
\var_A[A] = \var_B[\E_{A|B}[A|B]] + \E_B[\var_{A|B}[A|B]] .
\end{equation}\]</span> The subscripts on the inner expectations make it explicit that the expectations are being taken over the conditional distributions. <span class="math inline">\(\var_{A|B}[A|B]\)</span> and <span class="math inline">\(\E_{A|B}[A|B]\)</span> are random variables because the <span class="math inline">\(B\)</span> in the conditional is a random variable. We take the expectation or variance with <span class="math inline">\(B\)</span> fixed at one value, <span class="math inline">\(b\)</span>, but <span class="math inline">\(B\)</span> can take other values of <span class="math inline">\(b\)</span> also.</p>
<p>Going forward, I will write the law or total variance more succinctly as <span class="math display">\[\begin{equation}
\var[A] = \var_B[\E[A|B]] + \E_B[\var[A|B]] .
\end{equation}\]</span> I leave off the subscript on the inner conditional expectation or variance. Just remember that when you see a conditional in an expectation or variance, the integration is over over the conditional distribution of <span class="math inline">\(A\)</span> conditioned on <span class="math inline">\(B=b\)</span>. Even when you see <span class="math inline">\(A|B\)</span>, the conditioning is on <span class="math inline">\(B=b\)</span> and the <span class="math inline">\(B\)</span> indicates that this is a random variable because <span class="math inline">\(B\)</span> can take different <span class="math inline">\(b\)</span> values. When computing <span class="math inline">\(\var_B[\E_{A|B}[A|B]]\)</span>, we will typically compute <span class="math inline">\(\E_{A|b}[A|b]\)</span> and then compute (or infer) the variance or expectation of that over all possible values of <span class="math inline">\(b\)</span>.</p>
<p>The law of total variance will appear in this report in the following form: <span class="math display">\[\begin{equation}
\var_{XY}[f(\YY,\XX)] = \var_{Y^{(1)}}[\E_{XY|Y^{(1)}}[f(\YY,\XX)|\YY^{(1)}]] + \E_{Y^{(1)}}[\var_{XY|Y^{(1)}}[f(\YY,\XX)|\YY^{(1)}]] ,
\end{equation}\]</span> where <span class="math inline">\(f(\YY_t,\XX_t)\)</span> is some function of <span class="math inline">\(\XX_t\)</span> and <span class="math inline">\(\YY_t\)</span> and <span class="math inline">\(\YY^{(1)}\)</span> is the observed data from <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span> (<span class="math inline">\(\YY^{(2)}\)</span> is the unobserved data).</p>
<h2 id="model-residuals-conditioned-on-all-the-data">Model residuals conditioned on all the data</h2>
<p>Define the smoothations <span class="math inline">\(\hatvt\)</span> as: <span class="math display">\[\begin{equation}\label{eq:vtT}
\hatvt = \yy_t - \ZZ_t\hatxtT - \aa_t,
\end{equation}\]</span> where <span class="math inline">\(\hatxtT\)</span> is <span class="math inline">\(\E[\XX_t|\yy^{(1)}]\)</span>. The smoothation is different from <span class="math inline">\(\vv_t\)</span> because it uses <span class="math inline">\(\hatxtT\)</span> not <span class="math inline">\(\xx_t\)</span>; <span class="math inline">\(\xx_t\)</span> is not known, and <span class="math inline">\(\hatxtT\)</span> is its estimate. <span class="math inline">\(\hatxtT\)</span> is output by the Kalman smoother. <span class="math inline">\(\yy^{(1)}\)</span> means all the observed data from <span class="math inline">\(t=1\)</span> to <span class="math inline">\(T\)</span>. <span class="math inline">\(\yy^{(1)}\)</span> is a sample from the random variable <span class="math inline">\(\YY^{(1)}\)</span>. The unobserved <span class="math inline">\(\yy\)</span> will be termed <span class="math inline">\(\yy^{(2)}\)</span> and is a sample from the random variable <span class="math inline">\(\YY^{(2)}\)</span>. When <span class="math inline">\(\YY\)</span> appears without a superscript, it means both <span class="math inline">\(\YY^{(1)}\)</span> and <span class="math inline">\(\YY^{(2)}\)</span> together. Similarly <span class="math inline">\(\yy\)</span> means both <span class="math inline">\(\yy^{(1)}\)</span> and <span class="math inline">\(\yy^{(2)}\)</span> together—the observed data that we use to estimate <span class="math inline">\(\hatxtT\)</span> and the unobserved data that we do not use and may or may not know. <span class="math inline">\(\hatvt\)</span> exists for both <span class="math inline">\(\yy^{(1)}\)</span> and <span class="math inline">\(\yy^{(2)}\)</span>, though we might not know <span class="math inline">\(\yy^{(2)}\)</span> and thus might not know its corresponding <span class="math inline">\(\hatvt\)</span>. In some cases, however, we do know <span class="math inline">\(\yy^{(2)}\)</span>; they are data that we left out of our model fitting, in say a k-fold or leave-one-out cross-validation.</p>
<p><span class="math inline">\(\hatvt\)</span> is a sample from the random variable <span class="math inline">\(\hatVt\)</span>. We want to compute the mean and variance of this random variable over all possibles values that <span class="math inline">\(\XX_t\)</span> and <span class="math inline">\(\YY_t\)</span> might take. The mean of <span class="math inline">\(\hatVt\)</span> is 0 and we are concerned only with computing the variance: <span class="math display">\[\begin{equation}\label{eq:var.vtT}
\var[\hatVt] = \var_{XY}[\YY_t - \ZZ_t\E[\XX_t|\YY^{(1)}] - \aa_t] .
\end{equation}\]</span> Notice we have an unconditional variance over <span class="math inline">\({\XX,\YY}\)</span> on the outside and a conditional expectation over a specific value of <span class="math inline">\(\YY^{(1)}\)</span> on the inside (in the <span class="math inline">\(\E[\;]\)</span>).</p>
<p>From the law of total variance (Equation <span class="math inline">\(\ref{eq:lawoftotvar}\)</span>), we can write the variance of the model residuals as <span class="math display">\[\begin{equation}\label{eq:varvvtgeneral}
\var[\hatVt] = \var_{Y^{(1)}}[\E[\hatVt|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\hatVt|\YY^{(1)}]] .
\end{equation}\]</span></p>
<h3 id="first-term-on-right-hand-side-of-equation">First term on right hand side of Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span></h3>
<p>The random variable inside the <span class="math inline">\(\var[\;]\)</span> in the first term is <span class="math display">\[\begin{equation}\label{eq:first.term.rhs.varvvtgeneral}
\E[\hatVt|\YY^{(1)}]= \E[(\YY_t + \ZZ_t \E[\XX_t|\YY^{(1)}] + \aa_t)|\YY^{(1)}] .
\end{equation}\]</span> Let’s consider this for a specific value <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span>. <span class="math display">\[\begin{equation}
\E[\hatVt|\yy^{(1)}] = \E[(\YY_t + \ZZ_t \E[\XX_t|\yy^{(1)}] + \aa_t)|\yy^{(1)}] =
\E[\YY_t|\yy^{(1)}] + \ZZ_t \E[\E[\XX_t|\yy^{(1)}]|\yy^{(1)}] + \E[\aa_t|\yy^{(1)}] .
\end{equation}\]</span> <span class="math inline">\(\E[\XX_t|\yy^{(1)}]\)</span> is a fixed value, and the expected value of a fixed value is itself. So <span class="math inline">\(\E[\E[\XX_t|\yy^{(1)}]|\yy^{(1)}]=\E[\XX_t|\yy^{(1)}]\)</span>. Thus, <span class="math display">\[\begin{equation}
\E[\hatVt|\yy^{(1)}] = \E[\YY_t|\yy^{(1)}] + \ZZ_t \E[\XX_t|\yy^{(1)}] + \E[\aa_t|\yy^{(1)}] .
\end{equation}\]</span> We can move the conditional out and write <span class="math display">\[\begin{equation}
\E[\hatVt|\yy^{(1)}]= \E[(\YY_t + \ZZ_t \XX_t + \aa_t)|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}].
\end{equation}\]</span> The right side is <span class="math inline">\(\E[\VV_t|\yy^{(1)}]\)</span>, no hat on the <span class="math inline">\(\VV_t\)</span>, and this applies for all <span class="math inline">\(\yy^{(1)}\)</span>. This means that the first term in Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span> can be written with no hat on <span class="math inline">\(\VV\)</span>: <span class="math display">\[\begin{equation}\label{eq:no.hat.on.V}
\var_{Y^{(1)}}[\E[\hatVt|\YY^{(1)}]] = \var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] .
\end{equation}\]</span> If <span class="math inline">\(\YY_t\)</span> were completely observed (no missing values), this would be zero since <span class="math inline">\(\E[\VV_t|\yy]\)</span> would be a fixed value in that case. But <span class="math inline">\(\YY_t\)</span> is not assumed to be fully observed; it may have <span class="math inline">\(\YY^{(2)}_t\)</span> which is unobserved, or more precisely, not included in the estimation of <span class="math inline">\(VV_t\)</span> for whatever reason (“unknown” because it was unobserved being one reason). The derivation of <span class="math inline">\(\E[\YY_t|\yy^{(1)}]\)</span> is given in <span class="citation" data-cites="Holmes2010">Holmes (<a href="#ref-Holmes2010" role="doc-biblioref">2012</a>)</span>.</p>
<p>Using the law of total variance, we can re-write <span class="math inline">\(\var[\VV_t]\)</span> as: <span class="math display">\[\begin{equation}\label{eq:varianceVt}
\var[\VV_t] = \var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]].
\end{equation}\]</span> From Equation <span class="math inline">\(\ref{eq:varianceVt}\)</span>, we can solve for <span class="math inline">\(\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]]\)</span>: <span class="math display">\[\begin{equation}\label{eq:var.E.vtT}
\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] = \var[\VV_t] - \E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]].
\end{equation}\]</span> From Equation <span class="math inline">\(\ref{eq:unconditiondistofVt}\)</span>, we know that <span class="math inline">\(\var[\VV_t]=\RR_t\)</span> (this is the unconditional variance). Thus, <span class="math display">\[\begin{equation}\label{eq:var.E.vtT.R}
\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}]] = \RR_t - \E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]].
\end{equation}\]</span></p>
<p>The second term in Equation <span class="math inline">\(\ref{eq:var.E.vtT.R}\)</span> to the right of the equal sign and inside the expectation is <span class="math inline">\(\var[\VV_t|\YY^{(1)}]\)</span>. This is the variance of <span class="math inline">\(\VV_t\)</span> with <span class="math inline">\(\YY^{(1)}\)</span> held at a specific fixed <span class="math inline">\(\yy^{(1)}\)</span>. The variability in <span class="math inline">\(\var[\VV_t|\yy^{(1)}]\)</span> (notice <span class="math inline">\(\yy^{(1)}\)</span> not <span class="math inline">\(\YY^{(1)}\)</span> now) comes from <span class="math inline">\(\XX_t\)</span> and <span class="math inline">\(\YY^{(2)}\)</span> which are random variables. Let’s compute this variance for a specific <span class="math inline">\(\yy^{(1)}\)</span> value. <span class="math display">\[\begin{equation}\label{eq:varvtcondy}
\var[\VV_t|\yy^{(1)}] = \var[ \YY_t - \ZZ_t\XX_t-\aa_t | \yy^{(1)} ].
\end{equation}\]</span> Notice that there is no <span class="math inline">\(\E\)</span> (expectation) on the <span class="math inline">\(\XX_t\)</span>; this is <span class="math inline">\(\VV_t\)</span> not <span class="math inline">\(\hatVt\)</span>. <span class="math inline">\(\aa_t\)</span> is a fixed value and can be dropped.</p>
<p>Equation <span class="math inline">\(\ref{eq:varvtcondy}\)</span> can be written as: <span class="math display">\[\begin{equation}\label{eq:var.Vt.yy}
\begin{split}
\var[\VV_t|\yy^{(1)}] &amp;= \var[ \YY_t - \ZZ_t\XX_t | \yy^{(1)} ]\\
&amp;=\var[ - \ZZ_t\XX_t | \yy^{(1)} ] + \var[ \YY_t|\yy^{(1)}] + \cov[ \YY_t, - \ZZ_t\XX_t | \yy^{(1)} ] + \cov[ - \ZZ_t\XX_t, \YY_t | \yy^{(1)} ]\\
&amp;=\ZZ_t \hatVtT \ZZ_t^\top + \hatUtT - \hatStT\ZZ_t^\top - \ZZ_t(\hatStT)^\top .
\end{split}
\end{equation}\]</span> <span class="math inline">\(\hatVtT = \var[ \XX_t | \yy^{(1)} ]\)</span> and is output by the Kalman smoother. <span class="math inline">\(\hatUtT=\var[\YY_t|\yy^{(1)}]\)</span> and <span class="math inline">\(\hatStT=\cov[\YY_t,\XX_t|\yy^{(1)}]\)</span>. The equations for these are given in <span class="citation" data-cites="Holmes2010">Holmes (<a href="#ref-Holmes2010" role="doc-biblioref">2012</a>)</span> and are output by the <code>MARSShatyt()</code> function in the {MARSS} package. If there were no missing data, i.e., if <span class="math inline">\(\yy^{(1)}=\yy\)</span>, then <span class="math inline">\(\hatUtT\)</span> and <span class="math inline">\(\hatStT\)</span> would be zero because <span class="math inline">\(\YY_t\)</span> would be fixed at <span class="math inline">\(\yy_t\)</span>. This would reduce Equation <span class="math inline">\(\ref{eq:var.Vt.yy}\)</span> to <span class="math inline">\(\ZZ_t \hatVtT \ZZ_t^\top\)</span>. But we are concerned with the case where there are missing values. Those missing values need not be for all <span class="math inline">\(t\)</span>. That is, there may be some observed <span class="math inline">\(y\)</span> at time t and some missing <span class="math inline">\(y\)</span>. <span class="math inline">\(\yy_t\)</span> is multivariate.</p>
<p>From Equation <span class="math inline">\(\ref{eq:var.Vt.yy}\)</span>, we know <span class="math inline">\(\var[\VV_t|\yy^{(1)}]\)</span> for a specific <span class="math inline">\(\yy^{(1)}\)</span>. We want <span class="math inline">\(\E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]]\)</span> which is its expected value over all possible values of <span class="math inline">\(\yy^{(1)}\)</span>. <span class="math inline">\(\VV_t\)</span> is a multivariate normal random variable with two random variables <span class="math inline">\(\YY^{(1)}\)</span> and <span class="math inline">\(\YY^{(2)}\)</span>. The conditional variance of a multivariate Normal does not depend on the value that you are conditioning on. Let the <span class="math inline">\(\AA\)</span> be a N-dimensional multivariate Normal random variable partitioned into <span class="math inline">\(\AA_1\)</span> and <span class="math inline">\(\AA_2\)</span> with variance-covariance matrix <span class="math inline">\(\Sigma = \begin{bmatrix}
\Sigma_1 &amp; \Sigma_{12} \\
\Sigma_21 &amp; \Sigma_{2}
\end{bmatrix}\)</span>. The variance-covariance matrix of <span class="math inline">\(\AA\)</span> conditioned on <span class="math inline">\(\AA_1=\aa\)</span> is <span class="math inline">\(\Sigma = \begin{bmatrix}
0 &amp; 0 \\
0 &amp; \Sigma_2 - \Sigma_{12}\Sigma_{1}\Sigma_{21}
\end{bmatrix}\)</span>. Notice that <span class="math inline">\(\aa\)</span> does not appear in the conditional variance matrix. This means that <span class="math inline">\(\var[\VV_t|\yy^{(1)}]\)</span> does not depend on <span class="math inline">\(\yy^{(1)}\)</span>. Its variance only depends on the MARSS model parameters.</p>
<p>Because <span class="math inline">\(\var[\VV_t|\yy^{(1)}]\)</span> only depends on the MARSS parameters values, <span class="math inline">\(\QQ\)</span>, <span class="math inline">\(\BB\)</span>, <span class="math inline">\(\RR\)</span>, etc., the second term in Equation <span class="math inline">\(\ref{eq:var.E.vtT}\)</span>, <span class="math inline">\(\E_{Y^{(1)}}[\var[\VV_t|\YY^{(1)}]]\)</span>, is equal to <span class="math inline">\(\var[\VV_t|\yy^{(1)}]\)</span> (Equation <span class="math inline">\(\ref{eq:var.Vt.yy}\)</span>). Putting this into Equation <span class="math inline">\(\ref{eq:var.E.vtT.R}\)</span>, we have <span class="math display">\[\begin{equation}\label{eq:conditionalvtfinala}
\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)})]]  = \RR_t - \var[\VV_t|\yy^{(1)}] = \RR_t - \ZZ_t \hatVtT \ZZ_t^\top - \hatUtT + \hatStT\ZZ_t^\top + \ZZ_t(\hatStT)^\top.
\end{equation}\]</span> Since <span class="math inline">\(\var_{Y^{(1)}}[\E[\VV_t|\YY^{(1)})]] = \var_{Y^{(1)}}[\E[\hatVt|\YY^{(1)})]]\)</span> (Equation <span class="math inline">\(\ref{eq:no.hat.on.V}\)</span>), this means that the first term in Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span> is <span class="math display">\[\begin{equation}\label{eq:conditionalvtfinal}
\var_{Y^{(1)}}[\E[\hatVt|\YY^{(1)})]]  =  \RR_t - \ZZ_t \hatVtT \ZZ_t^\top - \hatUtT + \hatStT\ZZ_t^\top + \ZZ_t(\hatStT)^\top.
\end{equation}\]</span></p>
<h3 id="second-term-on-right-hand-side-of-equation">Second term on right hand side of Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span></h3>
<p>Consider the second term in Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span>. This term is <span class="math display">\[\begin{equation}\label{eq:second.term.rhs.9}
\E_{Y^{(1)}}[\var[\hatVt|\YY^{(1)}]] = \E_{Y^{(1)}}[\var[(\YY_t-\ZZ_t\E[\XX_t|\YY^{(1)}]-\aa_t)|\YY^{(1)}]] .
\end{equation}\]</span> The middle term is: <span class="math display">\[\begin{equation}
\E_{Y^{(1)}}[\var[\E[\XX_t|\YY^{(1)}]|\YY^{(1)}]].
\end{equation}\]</span> Let’s solve the inner part for a specific <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span>. <span class="math inline">\(\E[\XX_t|\yy^{(1)}]\)</span> is a fixed value. Thus <span class="math inline">\(\var[\E[\XX_t|\yy^{(1)}]|\yy^{(1)}]=0\)</span> since the variance of a fixed value is 0. This is true for all <span class="math inline">\(\yy^{(1)}\)</span> so the middle term reduces to 0. <span class="math inline">\(\aa_t\)</span> is also fixed and its variance is also 0. The covariance between a random variable and a fixed value is 0. Thus for a specific <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span>, the inside of the right hand side expectation reduces to <span class="math inline">\(\var[\YY_t|\yy^{(1)}]\)</span> which is <span class="math inline">\(\hatUtT\)</span>. As noted in the previous section, <span class="math inline">\(\hatUtT\)</span> is only a function of the MARSS parameters; it is not a function of <span class="math inline">\(\yy^{(1)}\)</span> and <span class="math inline">\(\var[\YY_t|\yy^{(1)}]=\hatUtT\)</span> for all <span class="math inline">\(\yy^{(1)}\)</span>. Thus the second term in Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span> is simply <span class="math inline">\(\hatUtT\)</span>: <span class="math display">\[\begin{equation}\label{eq:second.term.rhs.9final}
\E_{Y^{(1)}}[\var[\hatVt|\YY^{(1)}]] = \var[\hatVt|\yy^{(1)}] = \hatUtT .
\end{equation}\]</span></p>
<h3 id="putting-together-the-first-and-second-terms">Putting together the first and second terms</h3>
<p>We can now put the first and second terms in Equation <span class="math inline">\(\ref{eq:varvvtgeneral}\)</span> together (Equations <span class="math inline">\(\ref{eq:conditionalvtfinal}\)</span> and <span class="math inline">\(\ref{eq:second.term.rhs.9final}\)</span>) and write out the variance of the model residuals: <span class="math display">\[\begin{equation}\label{eq:first.and.secons.vvtgeneral}
\begin{split}
\var[\hatVt] &amp;= \RR_t - \ZZ_t \hatVtT \ZZ_t^\top - \hatUtT + \hatStT\ZZ_t^\top + \ZZ_t(\hatStT)^\top + \hatUtT\\
&amp;= \RR_t - \ZZ_t \hatVtT \ZZ_t^\top + \hatStT\ZZ_t^\top + \ZZ_t(\hatStT)^\top .
\end{split}
\end{equation}\]</span> Equation <span class="math inline">\(\ref{eq:first.and.secons.vvtgeneral}\)</span> will reduce to <span class="math inline">\(\RR_t - \ZZ_t \hatVtT \ZZ_t^\top\)</span> if <span class="math inline">\(\yy_t\)</span> has no missing values since <span class="math inline">\(\hatStT = 0\)</span> in this case. If <span class="math inline">\(\yy_t\)</span> is all missing values, <span class="math inline">\(\hatStT = \ZZ_t \hatVtT\)</span> because <span class="math display">\[\begin{equation}\label{eq:cov.Yt.Xt.no.missing.vals}
\cov[\YY_t, \XX_t|\yy^{(1)}] = \cov[\ZZ_t \XX_t + \aa_t + \VV_t, \XX_t|\yy^{(1)}] = \cov[\ZZ_t \XX_t, \XX_t|\yy^{(1)}] = \ZZ_t \cov[\XX_t, \XX_t|\yy^{(1)}] = \ZZ_t \hatVtT .
\end{equation}\]</span> The reduction in Equation <span class="math inline">\(\ref{eq:cov.Yt.Xt.no.missing.vals}\)</span> occurs because <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span> and by extension <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\XX_t\)</span> are independent in the form of MARSS model used in this report (Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>). Thus when <span class="math inline">\(\yy_t\)</span> is all missing values, Equation <span class="math inline">\(\ref{eq:first.and.secons.vvtgeneral}\)</span> will reduce to <span class="math inline">\(\RR_t + \ZZ_t \hatVtT \ZZ_t^\top\)</span> . The behavior if <span class="math inline">\(\yy_t\)</span> has some missing and some not missing values depends on whether <span class="math inline">\(\RR_t\)</span> is a diagonal matrix or not, i.e., if the <span class="math inline">\(\yy_t^{(1)}\)</span> and <span class="math inline">\(\yy_t^{(2)}\)</span> are correlated.</p>
<h2 id="state-residuals-conditioned-on-the-data">State residuals conditioned on the data</h2>
<p>The state residuals are <span class="math inline">\(\xx_t - (\BB_t \xx_{t-1} + \uu_t)=\ww_t\)</span>. The unconditional expected value of the state residuals is <span class="math inline">\(\E[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \E[\WW_t] = 0\)</span> and the unconditional variance of the state residuals is <span class="math display">\[\begin{equation}
\var[\XX_t - (\BB_t \XX_{t-1} + \uu_t)] = \var[\WW_t] = \QQ_t
\end{equation}\]</span> based on the definition of <span class="math inline">\(\WW_t\)</span> in Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>. The conditional state residuals (conditioned on the data from <span class="math inline">\(t=1\)</span> to <span class="math inline">\(t=T\)</span>) are defined as <span class="math display">\[\begin{equation}
\hatwt = \hatxtT - \BB_t\hatxtmT - \uu_t.
\end{equation}\]</span> where <span class="math inline">\(\hatxtT = E[\XX_t|\yy^{(1)}]\)</span> and <span class="math inline">\(\hatxtmT = E[\XX_{t-1}|\yy^{(1)}]\)</span>. <span class="math inline">\(\hatwt\)</span> is a sample from the random variable <span class="math inline">\(\hatWt\)</span>; random over different possible data sets. The expected value of <span class="math inline">\(\hatWt\)</span> is 0, and we are concerned with computing its variance.</p>
<p>We can write the variance of <span class="math inline">\(\WW_t\)</span> (no hat) using the law of total variance. <span class="math display">\[\begin{equation}\label{eq:Wlawoftotvar}
\var[\WW_t] = \var_{Y^{(1)}}[\E[\WW_t|\YY^{(1)}]] + \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]] .
\end{equation}\]</span> Notice that <span class="math display">\[\begin{equation}
\E[\WW_t|\yy^{(1)}] = \E[(\XX_t - \BB_t \XX_{t-1} - \uu_t)|\yy^{(1)}] =  \hatxtT - \BB_t \hatxtmT - \uu_t = \E[\hatWt|\yy^{(1)}] = \hatwt .
\end{equation}\]</span> This is true for all <span class="math inline">\(\yy^{(1)}\)</span>, thus <span class="math inline">\(\E[\WW_t|\YY^{(1)}]\)</span> is <span class="math inline">\(\hatWt\)</span>, and <span class="math inline">\(\var_{Y^{(1)}}[\E[\WW_t|\YY^{(1)}]] = \var[\hatWt]\)</span>. Equation <span class="math inline">\(\ref{eq:Wlawoftotvar}\)</span> can thus be written <span class="math display">\[\begin{equation}
\var[\WW_t] = \var[\hatWt] + \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]] .
\end{equation}\]</span> Solve for <span class="math inline">\(\var[\hatWt]\)</span>: <span class="math display">\[\begin{equation}\label{eq:varwwt}
\var[\hatWt] = \var[\WW_t] - \E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]].
\end{equation}\]</span></p>
<p>The variance in the expectation on the far right for a specific <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span> is <span class="math display">\[\begin{equation}
\var[\WW_t|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}-\uu_t) | \yy^{(1)} ] .
\end{equation}\]</span> <span class="math inline">\(\uu_t\)</span> is not a random variable and can be dropped. Thus, <span class="math display">\[\begin{equation}\label{eq:var.W.cond.y1}
\begin{split}
\var[\WW_t&amp;|\yy^{(1)}] = \var[ (\XX_t - \BB_t\XX_{t-1}) | \yy^{(1)} ] \\
&amp; = \var[ \XX_t | \yy^{(1)} ] + \var[\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[\XX_t, -\BB_t\XX_{t-1} | \yy^{(1)} ] + \cov[ -\BB_t\XX_{t-1}, \XX_t | \yy^{(1)} ]\\
&amp; = \hatVtT + \BB_t \hatVtmT \BB_t^\top - \hatVttmT\BB_t^\top - \BB_t\hatVtmtT .
\end{split}
\end{equation}\]</span> Again this is conditional multivariate normal variance, and its value does not depend on the value, <span class="math inline">\(\yy^{(1)}\)</span> that we are conditioning on. It depends only on the parameters values, <span class="math inline">\(\QQ\)</span>, <span class="math inline">\(\BB\)</span>, <span class="math inline">\(\RR\)</span>, etc., and is the same for all values of <span class="math inline">\(\yy^{(1)}\)</span>. So <span class="math inline">\(\E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]] = \var[\WW_t|\yy^{(1)}]\)</span>, using any value of <span class="math inline">\(\yy^{(1)}\)</span>. Thus <span class="math display">\[\begin{equation}\label{eq:E.var.Wt.yt}
\E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]] =  \hatVtT + \BB_t\hatVtmT\BB_t^\top - \hatVttmT\BB_t^\top - \BB_t\hatVtmtT .
\end{equation}\]</span></p>
<p>Putting <span class="math inline">\(\E_{Y^{(1)}}[\var[\WW_t|\YY^{(1)}]]\)</span> from Equation <span class="math inline">\(\ref{eq:E.var.Wt.yt}\)</span> and <span class="math inline">\(\var[\WW_t]=\QQ_t\)</span> into Equation <span class="math inline">\(\ref{eq:varwwt}\)</span>, the variance of the conditional state residuals is <span class="math display">\[\begin{equation}
\var[\hatWt] = \QQ_t - \hatVtT - \BB_t\hatVtmT\BB_t^\top + \hatVttmT\BB_t^\top + \BB_t\hatVtmtT .
\end{equation}\]</span></p>
<h2 id="covariance-of-the-conditional-model-and-state-residuals">Covariance of the conditional model and state residuals</h2>
<p>The unconditional model and state residuals, <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span>, are independent by definition (in Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>), i.e., <span class="math inline">\(\cov[\VV_t,\WW_t]=0\)</span>. However the conditional model and state residuals, <span class="math inline">\(\cov[\hatVt,\hatWt]\)</span>, are not independent since both depend on <span class="math inline">\(\yy^{(1)}\)</span>.<br />
Using the law of total covariance, we can write <span class="math display">\[\begin{equation}\label{eq:covhatVtWt1}
\cov[\hatVt,\hatWt] =
\cov_{Y^{(1)}}[\E[\hatVt|\YY^{(1)}],\E[\hatWt|\YY^{(1)}]] + \E_{Y^{(1)}}[\cov[\hatVt, \hatWt|\YY^{(1)}]] .
\end{equation}\]</span></p>
<p>For a specific value of <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span>, the covariance in the second term on the right is <span class="math inline">\(\cov[\hatVt, \hatWt|\yy^{(1)}]\)</span>. Conditioned on a specific value of <span class="math inline">\(\YY^{(1)}\)</span>, <span class="math inline">\(\hatWt\)</span> is a fixed value, <span class="math inline">\(\hatwt = \hatxtT - \BB_t\hatxtmT - \uu_t\)</span>, and conditioned on <span class="math inline">\(\yy^{(1)}\)</span>, <span class="math inline">\(\hatxtT\)</span> and <span class="math inline">\(\hatxtmT\)</span> are fixed values. <span class="math inline">\(\uu_t\)</span> is also fixed; it is a parameter. <span class="math inline">\(\hatVt\)</span> is not a fixed value because it has <span class="math inline">\(\YY_t^{(2)}\)</span> and that is a random variable. Thus <span class="math inline">\(\cov[\hatVt, \hatWt|\yy^{(1)}]\)</span> is the covariance between a random variable and a fixed variable and thus the covariance is 0. This is true for all <span class="math inline">\(\yy^{(1)}\)</span>. Thus the second right-side term in Equation <span class="math inline">\(\ref{eq:covhatVtWt1}\)</span> is zero, and the equation reduces to <span class="math display">\[\begin{equation}\label{eq:covhatVtWt3}
\cov[\hatVt,\hatWt] = \cov_{Y^{(1)}}[\E[\hatVt|\YY^{(1)}],\E[\hatWt|\YY^{(1)}]].
\end{equation}\]</span> Notice that <span class="math inline">\(\E[\hatWt|\yy^{(1)}]=\E[\WW_t|\yy^{(1)}]\)</span> and <span class="math inline">\(\E[\hatVt|\yy^{(1)}]=\E[\VV_t|\yy^{(1)}]\)</span> since <span class="math display">\[\begin{equation}
\E[\WW_t|\yy^{(1)}]= \E[\XX_t|\yy^{(1)}]-\BB_t\E[\XX_{t-1}|\yy^{(1)}] - \uu_t = \hatxtT-\BB_t\hatxtmT - \uu_t = \hatwt = \E[\hatWt|\yy^{(1)}]
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\E[\VV_t|\yy^{(1)}]= \E[\YY_t|\yy^{(1)}]-\ZZ_t\E[\XX_{t}|\yy^{(1)}] - \aa_t = \E[\YY_t|\yy^{(1)}]-\ZZ_t \hatxtT - \aa_t = \E[\hatVt|\yy^{(1)}] .
\end{equation}\]</span> Thus the right side of Equation <span class="math inline">\(\ref{eq:covhatVtWt3}\)</span> can be written in terms of <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span> instead of <span class="math inline">\(\hatVt\)</span> and <span class="math inline">\(\hatWt\)</span>: <span class="math display">\[\begin{equation}\label{eq:covhatVtWt2}
\cov[\hatVt,\hatWt] = \cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_t|\YY^{(1)}]].
\end{equation}\]</span></p>
<p>Using the law of total covariance, we can write: <span class="math display">\[\begin{equation}\label{eq:covVtWt}
\cov[\VV_t, \WW_t] = \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\YY^{(1)}]] + \cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_t|\YY^{(1)}]] .
\end{equation}\]</span> The unconditional covariance of <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span> is 0. Thus the left side of Equation <span class="math inline">\(\ref{eq:covVtWt}\)</span> is 0 and we can rearrange the equation as <span class="math display">\[\begin{equation}\label{eq:covVtWt2}
\cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_t|\YY^{(1)}]] = - \E_{Y^{(1)}}[\cov[\VV_t, \WW_t|\YY^{(1)}]] .
\end{equation}\]</span> Combining Equation <span class="math inline">\(\ref{eq:covhatVtWt2}\)</span> and <span class="math inline">\(\ref{eq:covVtWt2}\)</span>, we get <span class="math display">\[\begin{equation}\label{eq:conditionalcovvtwt}
\cov[\hatVt,\hatWt] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\YY^{(1)}] ] ,
\end{equation}\]</span> and our problem reduces to solving for the conditional covariance of the model and state residuals (right side of Equation <span class="math inline">\(\ref{eq:conditionalcovvtwt}\)</span>).</p>
<p>For a specific <span class="math inline">\(\YY^{(1)}=\yy^{(1)}\)</span>, the conditional covariance <span class="math inline">\(\cov[\VV_t, \WW_t|\yy^{(1)}]\)</span> can be written out as <span class="math display">\[\begin{equation}
\cov[\VV_t, \WW_t|\yy^{(1)}] = \cov[\YY_t-\ZZ_t\XX_t-\aa_t,\, \XX_t-\BB_t\XX_{t-1}-\uu_t|\yy^{(1)}] .
\end{equation}\]</span> <span class="math inline">\(\aa_t\)</span> and <span class="math inline">\(\uu_t\)</span> are fixed values and can be dropped. Thus <span class="math display">\[\begin{equation}
\begin{split}
\cov&amp;[\VV_t, \WW_t|\yy^{(1)}] =\cov[\YY_t-\ZZ_t\XX_t, \XX_t-\BB_t\XX_{t-1}|\yy^{(1)}] \\
&amp; =\cov[\YY_t,\XX_t|\yy^{(1)}] + \cov[\YY_t,-\BB_t\XX_{t-1}|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,\XX_t|\yy^{(1)}] + \cov[-\ZZ_t\XX_t,-\BB_t\XX_{t-1}|\yy^{(1)}]\\
&amp; = \hatStT - \hatSttmT\BB_t^\top - \ZZ_t \hatVtT + \ZZ_t\hatVttmT\BB_t^\top ,
\end{split}
\end{equation}\]</span> where <span class="math inline">\(\hatStT=\cov[\YY_t,\XX_t|\yy^{(1)}]\)</span> and <span class="math inline">\(\hatSttmT=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]\)</span>; the equations for <span class="math inline">\(\hatStT\)</span> and <span class="math inline">\(\hatSttmT\)</span> are given in <span class="citation" data-cites="Holmes2010">Holmes (<a href="#ref-Holmes2010" role="doc-biblioref">2012</a>)</span> and are output by the <code>MARSShatyt()</code> function in the {MARSS} package.</p>
<p>Both <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span> are multivariate normal random variables that depend on <span class="math inline">\(\YY^{(1)}\)</span> and <span class="math inline">\(\YY^{(2)}\)</span> and the conditional covariance is not a function of the variable that we condition on (in this case <span class="math inline">\(\yy^{(1)}\)</span>). The conditional covariance is only a function of the MARSS parameters. Thus <span class="math display">\[\begin{equation}
\E_{Y^{(1)}}[ \cov[\VV_t, \WW_t|\YY^{(1)}] ]= \cov[\VV_t, \WW_t|\yy^{(1)}] = \hatStT - \hatSttmT\BB_t^\top - \ZZ_t \hatVtT + \ZZ_t\hatVttmT\BB_t^\top .
\end{equation}\]</span> <span class="math inline">\(\cov[\hatVt,\hatWt]\)</span> is the negative of this (Equation <span class="math inline">\(\ref{eq:conditionalcovvtwt}\)</span>), thus <span class="math display">\[\begin{equation}
\cov[\hatVt,\hatWt] = - \hatStT + \hatSttmT\BB_t^\top + \ZZ_t \hatVtT - \ZZ_t\hatVttmT\BB_t^\top .
\end{equation}\]</span></p>
<p>The Harvey et al. algorithm (next section) gives the joint distribution of the model residuals at time <span class="math inline">\(t\)</span> and state residuals at time <span class="math inline">\(t+1\)</span>. Using the law of total covariance as above, the covariance in this case is <span class="math display">\[\begin{equation}
\cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_{t+1}|\YY^{(1)}]] = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\YY^{(1)}] ]
\end{equation}\]</span> and <span class="math display">\[\begin{equation}
\begin{split}
\cov[\VV_t, \WW_{t+1}|\yy^{(1)}] &amp; =\cov[\YY_t-\ZZ_t\XX_t-\aa_t,\, \XX_{t+1}-\BB_{t+1}\XX_t-\uu_{t+1}|\yy^{(1)}] \\
&amp; =\cov[\YY_t-\ZZ_t\XX_t,\, \XX_{t+1}-\BB_{t+1}\XX_t|\yy^{(1)}] \\
&amp; = \hatSttpT - \hatStT\BB_{t+1}^\top - \ZZ_t\hatVttpT + \ZZ_t \hatVtT \BB_{t+1}^\top .
\end{split}
\end{equation}\]</span> Thus, <span class="math display">\[\begin{equation}
\begin{split}
\cov_{Y^{(1)}}[\E[\VV_t|\YY^{(1)}],\E[\WW_{t+1}|\YY^{(1)}]] &amp; = - \E_{Y^{(1)}}[ \cov[\VV_t, \WW_{t+1}|\YY^{(1)}] ] \\
&amp; = - \hatSttpT + \hatStT\BB_{t+1}^\top + \ZZ_t\hatVttpT - \ZZ_t\hatVtT\BB_{t+1}^\top.
\end{split}
\end{equation}\]</span></p>
<h2 id="joint-distribution-of-the-conditional-residuals">Joint distribution of the conditional residuals</h2>
<p>We now can write the variance of the joint distribution of the conditional residuals. Define <span class="math display">\[\begin{equation}
\widehat{\varepsilon}_t = \begin{bmatrix}\hatvt\\ \hatwt\end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxtT - \aa_t\\ \hatxtT - \BB_t\hatxtmT - \uu_t \end{bmatrix}.
\end{equation}\]</span> <span class="math inline">\(\widehat{\varepsilon}_t\)</span> is a sample drawn from the distribution of the random variable <span class="math inline">\(\widehat{\mathcal{E}}_t\)</span>. The expected value of <span class="math inline">\(\widehat{\mathcal{E}}_t\)</span> over all possible <span class="math inline">\(\yy\)</span> is 0 and the variance of <span class="math inline">\(\widehat{\mathcal{E}}_t\)</span> is <span class="math display">\[\begin{equation}
\widehat{\Sigma}_t = \var[\widehat{\mathcal{E}}_t] = \begin{bmatrix}[c|c]
\var[\hatVt]&amp;
\cov[\hatVt, \hatWt] \\
\rule[.5ex]{20ex}{0.25pt} &amp; \rule[.5ex]{20ex}{0.25pt} \\
(\cov[\hatVt, \hatWt])^\top&amp;
\var[\hatWt] \end{bmatrix}
\end{equation}\]</span> which is <span class="math display">\[\begin{equation}\label{eq:jointcondresid1general}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t\hatVtT\ZZ_t^\top + \hatStT\ZZ_t^\top+\ZZ_t(\hatStT)^\top&amp;
- \hatStT + \hatSttmT\BB_t^\top + \ZZ_t\hatVtT - \ZZ_t\hatVttmT\BB_t^\top\\
  \rule[.5ex]{40ex}{0.25pt} &amp; \rule[.5ex]{50ex}{0.25pt} \\
  (- \hatStT + \hatSttmT\BB_t^\top + \ZZ_t\hatVtT  - \ZZ_t\hatVttmT\BB_t^\top)^\top&amp;
  \QQ_t - \hatVtT - \BB_t\hatVtmT\BB_t^\top + \hatVttmT\BB_t^\top + \BB_t\hatVtmtT \end{bmatrix},
  \end{equation}\]</span> where <span class="math inline">\(\hatStT=\cov[\YY_t,\XX_t|\yy^{(1)}]\)</span>, <span class="math inline">\(\hatSttmT=\cov[\YY_t,\XX_{t-1}|\yy^{(1)}]\)</span>, <span class="math inline">\(\hatVtT=\var[\XX_t|\yy^{(1)}]\)</span>, <span class="math inline">\(\hatVttmT=\cov[\XX_t,\XX_{t-1}|\yy^{(1)}]\)</span>, and <span class="math inline">\(\hatVtmtT=\cov[\XX_{t-1},\XX_t|\yy^{(1)}]\)</span>. This gives the variance of both “observed” model residuals (the ones associated with <span class="math inline">\(\yy^{(1)}\)</span>) and the unobserved model residuals (the ones associated with <span class="math inline">\(\yy^{(2)}\)</span>).<br />
When there are no missing values in <span class="math inline">\(\yy_t\)</span>, the <span class="math inline">\(\hatStT\)</span> and <span class="math inline">\(\hatSttmT\)</span> terms equal 0 and drop out.</p>
<p>If the residuals are defined as in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> with <span class="math inline">\(\hatvt\)</span> on top and <span class="math inline">\(\hatwtp\)</span> on the bottom instead of <span class="math inline">\(\hatwt\)</span>, then <span class="math display">\[\begin{equation}
\widehat{\varepsilon}_t = \begin{bmatrix}\hatvt\\ \hatwtp \end{bmatrix} =
\begin{bmatrix}\yy_t - \ZZ_t\hatxtT - \aa_t\\ \hatxtpT - \BB_{t+1}\hatxtT - \uu_{t+1} \end{bmatrix}
\end{equation}\]</span> and the variance of <span class="math inline">\(\widehat{\mathcal{E}}_t\)</span> is <span class="math display">\[\begin{equation}
\begin{bmatrix}[c|c]
\var[\hatVt]&amp;
\cov[\hatVt, \hatWtp] \\
\rule[.5ex]{20ex}{0.25pt} &amp; \rule[.5ex]{20ex}{0.25pt} \\
(\cov[\hatVt, \hatWtp])^\top&amp;
\var[\hatWtp] \end{bmatrix}
\end{equation}\]</span> which is <span class="math display">\[\begin{equation}\label{eq:jointcondresid2}
\begin{bmatrix}[c|c]
\RR_t - \ZZ_t\hatVtT\ZZ_t^\top + \hatStT\ZZ_t^\top + \ZZ_t(\hatStT)^\top&amp;
- \hatSttpT + \hatStT\BB_{t+1}^\top + \ZZ_t\hatVttpT - \ZZ_t\hatVtT\BB_{t+1}^\top \\
  \rule[.5ex]{40ex}{0.25pt} &amp; \rule[.5ex]{50ex}{0.25pt} \\
  (- \hatSttpT + \hatStT\BB_{t+1}^\top + \ZZ_t\hatVttpT - \ZZ_t\hatVtT\BB_{t+1}^\top)^\top&amp;
  \QQ_{t+1} - \hatVtpT - \BB_{t+1}\hatVtT\BB_{t+1}^\top + \hatVtptT \BB_{t+1}^\top + \BB_{t+1}\hatVttpT \end{bmatrix} .
  \end{equation}\]</span></p>
<h1 id="harvey-et-al.-1998-algorithm-for-the-conditional-residuals">Harvey et al. 1998 algorithm for the conditional residuals</h1>
<div style="display:none">
<span class="math inline">\(\nextSection\)</span>
</div>
<p>Pages 112-113 in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> give a recursive algorithm for computing the variance of the conditional residuals when the time-varying MARSS equation is written as: <span class="math display">\[\begin{equation}\label{eq:residsMARSSHarvey}
\begin{gathered}
\xx_{t+1} = \BB_{t+1}\xx_t + \uu_{t+1} + \HH_{t+1}\epsilon_{t},\\
\yy_t = \ZZ_t\xx_t + \aa_t + \GG_t\epsilon_t,\\
\mbox{ where } \epsilon_t \sim \MVN(0,\II_{m+n \times m+n}) \\
\HH_t\HH_t^\top=\QQ_t, \GG_t\GG_t^\top=\RR_t, \text{ and } \HH_t\GG_t^\top = \cov[\WW_t, \VV_t]
\end{gathered}
\end{equation}\]</span> The <span class="math inline">\(\HH_t\)</span> and <span class="math inline">\(\GG_t\)</span> matrices specify the variance and covariance of <span class="math inline">\(\WW_t\)</span> and <span class="math inline">\(\VV_t\)</span>. <span class="math inline">\(\HH_t\)</span> has <span class="math inline">\(m\)</span> rows and <span class="math inline">\(m+n\)</span> columns and <span class="math inline">\(\GG_t\)</span> has <span class="math inline">\(n\)</span> rows and <span class="math inline">\(m+n\)</span> columns. In the MARSS equation for this report (Equation <span class="math inline">\(\ref{eq:residsMARSS}\)</span>), <span class="math inline">\(\WW_t\)</span> and <span class="math inline">\(\VV_t\)</span> are independent. To achieve this in the Harvey et al. form (Equation <span class="math inline">\(\ref{eq:residsMARSSHarvey}\)</span>), the first <span class="math inline">\(n\)</span> columns of <span class="math inline">\(\HH_t\)</span> are all 0 and the last <span class="math inline">\(m\)</span> columns of <span class="math inline">\(\GG_t\)</span> are all zero.</p>
<p>The algorithm in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> gives the variance of the normalized residuals, the <span class="math inline">\(\epsilon_t\)</span>. I have modified their algorithm so it returns the non-normalized residuals: <span class="math display">\[\varepsilon_t=\begin{bmatrix}\GG_t\epsilon_t\\ \HH_{t+1}\epsilon_t\end{bmatrix}=\begin{bmatrix}\vv_t\\ \ww_{t+1} \end{bmatrix}.\]</span></p>
<p>The Harvey et al. algorithm is a backwards recursion using the following output from the Kalman filter: the one-step ahead prediction covariance <span class="math inline">\(\FF_t\)</span>, the Kalman gain <span class="math inline">\(\KK_t\)</span>, <span class="math inline">\(\hatxttm=\E[\XX_t|\yy^{(1),1:{t-1}}]\)</span> and <span class="math inline">\(\hatVttm=\var[\XX_t|\yy^{(1),1:{t-1}}]\)</span>. In the {MARSS} package, these are output from  in , ,  and .</p>
<h2 id="algorithm-for-the-non-normalized-residuals">Algorithm for the non-normalized residuals</h2>
<p>Start from <span class="math inline">\(t=T\)</span> and work backwards to <span class="math inline">\(t=1\)</span>. At time <span class="math inline">\(T\)</span>, <span class="math inline">\(r_T=0_{1 \times m}\)</span> and <span class="math inline">\(N_T=0_{m \times m}\)</span>. <span class="math inline">\(\BB_{t+1}\)</span> and <span class="math inline">\(\QQ_{t+1}\)</span> can be set to NA or 0. They will not appear in the algorithm at time <span class="math inline">\(T\)</span> since <span class="math inline">\(r_T=0\)</span> and <span class="math inline">\(N_T=0\)</span>. Note that the <span class="math inline">\(\ww\)</span> residual and its associated variance and covariance with <span class="math inline">\(\vv\)</span> at time <span class="math inline">\(T\)</span> is NA since this residual would be for <span class="math inline">\(\xx_T\)</span> to <span class="math inline">\(\xx_{T+1}\)</span>.</p>
<!--
% algorithm with \GG_t and \HH_t
% \begin{equation}\label{eq:Harveyalgo}
% \begin{gathered}
% \FF_t = \ZZ_t\hatVt\ZZ_t^\top+\RR_t\\
% G_t= \GG_t\QQ_t\GG_t^\top, \mbox{  }H_t = \HH_t\RR_t\HH_t^\top,  \mbox{ } K_t = \BB_t\KK^{*}_t\\
% L_t = \BB_t - K_t\ZZ_t, \mbox{ } J_t= H_t - K_t G_t, \mbox{ } u_t = \FF_t^{-1} - K_t^\top r_t\\
% r_{t-1} = \ZZ_t^\top u_t + \BB_t^\top r_t, \mbox{ } N_{t-1} = K_t^\top N_t K_t + L_t^\top N_t L_t
% \end{gathered}
% \end{equation}
-->
<p><span class="math display">\[\begin{equation}\label{eq:Harveyalgo}
\begin{gathered}
\QQ^\prime_{t+1}=\begin{bmatrix}0_{m \times n}&amp;\QQ_{t+1}\end{bmatrix}, \mbox{    } \RR^\prime_t=\begin{bmatrix}\RR_t^* &amp; 0_{n \times m}\end{bmatrix}\\
\FF_t = \ZZ_t^*\hatVttm{\ZZ_t^*}^\top+\RR_t^* , \,\, n \times n \\
K_t = \BB_{t+1}\KK_t = \BB_{t+1} \hatVttm{\ZZ_t^*}^\top \FF_t^{-1}  , \,\, m \times n  \\
L_t = \BB_{t+1} - K_t\ZZ_t^*  , \,\, m \times m \\
J_t= \QQ^\prime_{t+1} - K_t \RR^\prime_t  , \,\, m \times (n+m) \\
v_t = \yy_t^* - \ZZ_t\hatxttm - \aa_t , \,\, n \times 1 \\
u_t = \FF_t^{-1} v_t - K_t^\top r_t , \,\, n \times 1 \\
r_{t-1} = {\ZZ_t^*}^\top u_t + \BB_{t+1}^\top r_t , \,\, m \times 1  \\
N_{t-1} = {\ZZ_t^*}^\top \FF_t^{-1} \ZZ_t^* + L_t^\top N_t L_t   , \,\, m \times m .
\end{gathered}
\end{equation}\]</span> <span class="math inline">\(\yy_t^*\)</span> is the observed data at time <span class="math inline">\(t\)</span> with the <span class="math inline">\(i\)</span>-th rows set to 0 if the <span class="math inline">\(i\)</span>-th <span class="math inline">\(y\)</span> is missing. Bolded terms are the same as in Equation <span class="math inline">\(\ref{eq:residsMARSSHarvey}\)</span> (and are output by ). Unbolded terms are terms used in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span>. The * on <span class="math inline">\(\ZZ_t\)</span> and <span class="math inline">\(\RR_t\)</span>, indicates that they are the missing value modified versions discussed in section 6.4 of <span class="citation" data-cites="ShumwayStoffer2006">Shumway &amp; Stoffer (<a href="#ref-ShumwayStoffer2006" role="doc-biblioref">2006</a>)</span> and <span class="citation" data-cites="Holmes2010">Holmes (<a href="#ref-Holmes2010" role="doc-biblioref">2012</a>)</span>: to construct <span class="math inline">\(\ZZ_t^*\)</span> and <span class="math inline">\(\RR_t^*\)</span>, the rows of <span class="math inline">\(\ZZ_t\)</span> corresponding to missing rows of <span class="math inline">\(\yy_t\)</span> are set to zero and the <span class="math inline">\((i,j)\)</span> and <span class="math inline">\((j,i)\)</span> terms of <span class="math inline">\(\RR_t\)</span> corresponding the missing rows of <span class="math inline">\(\yy_t\)</span> are set to zero. For the latter, this means if the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\yy_t\)</span> is missing, then then the <span class="math inline">\(i\)</span>-th row and column (including the value on the diagonal) in <span class="math inline">\(\RR_t\)</span> are set to 0. Notice that <span class="math inline">\(\FF_t\)</span> will have 0’s on the diagonal if there are missing values. A modified inverse of <span class="math inline">\(\FF_t\)</span> is used: any 0’s on the diagonal of <span class="math inline">\(\FF_t\)</span> are replaced with 1, the inverse is taken, and 1s on diagonals is replaced back with 0s.</p>
<p>The residuals <span class="citation" data-cites="Harveyetal1998">(<a href="#ref-Harveyetal1998" role="doc-biblioref">Harvey et al., 1998</a>, eqn 24)</span> are <span class="math display">\[\begin{equation}\label{eq:Harveyresiduals}
\widehat{\varepsilon}_t = \begin{bmatrix}\hatvt\\ \hatwtp \end{bmatrix} =({\RR^\prime_t})^\top u_t + ({\QQ^\prime_{t+1}})^\top r_t
\end{equation}\]</span> The expected value of <span class="math inline">\(\widehat{\mathcal{E}}_t\)</span> is 0 and its variance is <span class="math display">\[\begin{equation}\label{eq:Harveyvariance}
\widehat{\Sigma]}_t = \var[\widehat{\mathcal{E}}_t] ={\RR^\prime_t}^\top \FF_t^{-1} \RR^\prime_t + J_t^\top N_t J_t .
\end{equation}\]</span> These <span class="math inline">\(\widehat{\varepsilon}_t\)</span> and <span class="math inline">\(\widehat{\Sigma]}_t\)</span> are for both the non-missing and missing <span class="math inline">\(\yy_t\)</span>. This is a modification to the <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> algorithm which does not give the variance for missing <span class="math inline">\(\yy\)</span>.</p>
<h2 id="difference-in-notation">Difference in notation</h2>
<p>In Equation 20 in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span>, their <span class="math inline">\(T_t\)</span> is my <span class="math inline">\(\BB_{t+1}\)</span> and their <span class="math inline">\(H_t H_t^\top\)</span> is my <span class="math inline">\(\QQ_{t+1}\)</span>. Notice the difference in the time indexing. My time indexing on <span class="math inline">\(\BB\)</span> and <span class="math inline">\(\QQ\)</span> matches the left <span class="math inline">\(\xx\)</span> while in theirs, <span class="math inline">\(T\)</span> and <span class="math inline">\(H\)</span> indexing matches the right <span class="math inline">\(\xx\)</span>. Thus in my implementation of their algorithm <span class="citation" data-cites="Harveyetal1998">(<a href="#ref-Harveyetal1998" role="doc-biblioref">Harvey et al., 1998</a>, eqns. 21-24)</span>, <span class="math inline">\(\BB_{t+1}\)</span> appears in place of <span class="math inline">\(T_t\)</span> and <span class="math inline">\(\QQ_{t+1}\)</span> appears in place of <span class="math inline">\(H_t\)</span>. See comments below on normalization and the difference between <span class="math inline">\(\QQ\)</span> and <span class="math inline">\(H\)</span>.</p>
<p><span class="citation" data-cites="Harveyetal1998">(<a href="#ref-Harveyetal1998" role="doc-biblioref">Harvey et al., 1998</a>, eqns. 19, 20)</span> use <span class="math inline">\(G_t\)</span> to refer to the <span class="math inline">\(\chol(\RR_t)^\top\)</span> (non-zero part of the <span class="math inline">\(n \times n+m\)</span> matrix) and <span class="math inline">\(H_t\)</span> to refer to <span class="math inline">\(\chol(\QQ_t)^\top\)</span>. I have replaced these with <span class="math inline">\(\RR_t^\prime\)</span> and <span class="math inline">\(\QQ_t^\prime\)</span> (Equation <span class="math inline">\(\ref{eq:Harveyalgo}\)</span>) which causes my variant of their algorithm (Equation <span class="math inline">\(\ref{eq:Harveyalgo}\)</span>) to give the non-normalized variance of the residuals. The residuals function in the {MARSS} package has an option to give either normalized or non-normalized residuals.</p>
<p><span class="math inline">\(\KK_t\)</span> is the Kalman gain output by the {MARSS} package <code>MARSSkf()</code> function. The Kalman gain as used in the <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> algorithm is <span class="math inline">\(K_t=\BB_{t+1}\KK_t\)</span>. Notice that Equation 21 in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> has <span class="math inline">\(H_t G_t^\top\)</span> in the equation for <span class="math inline">\(K_t\)</span>. This is the covariance of the state and observation errors, which is allowed to be non-zero given the way Harvey et al. write the errors in their Equations 19 and 20. The way the {MARSS} package model is written, the state and observation errors are independent of each other. Thus <span class="math inline">\(H_t G_t^\top = 0\)</span> and this term drops out of the <span class="math inline">\(K_t\)</span> equation in Equation <span class="math inline">\(\ref{eq:Harveyalgo}\)</span>.</p>
<h2 id="computing-the-normalized-residuals">Computing the normalized residuals</h2>
<p>To compute the normalized residuals and residuals variance, a block diagonal matrix with the inverse of the <span class="math inline">\(\RR\)</span> and <span class="math inline">\(\QQ\)</span> matrices is used. The normalized residuals are: <span class="math display">\[\begin{equation}
\begin{bmatrix}\RR^{-1}_t&amp;0\\0&amp;\QQ^{-1}_t\end{bmatrix} \widehat{\varepsilon}_t
\end{equation}\]</span> The variance of the normalized residuals is <span class="math display">\[\begin{equation}
\begin{bmatrix}\RR^{-1}_t&amp;0\\0&amp;\QQ^{-1}_t\end{bmatrix} \widehat{\Sigma]}_t \begin{bmatrix}\RR^{-1}_t&amp;0\\0&amp;\QQ^{-1}_t\end{bmatrix}
\end{equation}\]</span></p>
<h2 id="computing-the-cholesky-standardized-residuals">Computing the Cholesky standardized residuals</h2>
<p>The Cholesky standardized residuals are computed by multiplying <span class="math inline">\(\widehat{\varepsilon}_t\)</span> by <span class="math inline">\((\widehat{\Sigma}_t)^{-1/2}\)</span> (the inverse of the Cholesky decomposition of the variance-covariance matrix for <span class="math inline">\(\widehat{\varepsilon}_t\)</span>): <span class="math display">\[\begin{equation}\label{eq:std.resid}
\widehat{\varepsilon}_{t, std} = (\widehat{\Sigma}_t)^{-1/2}\widehat{\varepsilon}_t .
\end{equation}\]</span> These residuals are uncorrelated (across the residuals at time <span class="math inline">\(t\)</span> in <span class="math inline">\(\widehat{\varepsilon}_t\)</span>). See <span class="citation" data-cites="HarveyKoopman1992">Harvey &amp; Koopman (<a href="#ref-HarveyKoopman1992" role="doc-biblioref">1992</a>)</span> and section V in <span class="citation" data-cites="Harveyetal1998">Harvey et al. (<a href="#ref-Harveyetal1998" role="doc-biblioref">1998</a>)</span> for a discussion on how to use these residuals for outlier detection and structural break detection.</p>
<p>It is also common to use the marginal standardized residuals, which would be <span class="math inline">\(\widehat{\varepsilon}_t\)</span> multiplied by the inverse of the square-root of <span class="math inline">\(\dg(\widehat{\Sigma}_t)\)</span>, where <span class="math inline">\(\dg(\AA)\)</span> is a diagonal matrix formed from the diagonal of the square matrix <span class="math inline">\(\AA\)</span>. <span class="math display">\[\begin{equation}
\widehat{\varepsilon}_{t, mar} = \dg(\widehat{\Sigma}_t)^{-1/2}\widehat{\varepsilon}_t
\end{equation}\]</span> Marginal standardized residuals may be correlated at time <span class="math inline">\(t\)</span> (unlike the Cholesky standardized residuals) but are a bit easier to interpret when there is correlation across the model residuals.</p>
<h1 id="distribution-of-the-marss-innovation-residuals">Distribution of the MARSS innovation residuals</h1>
<div style="display:none">
<span class="math inline">\(\nextSection\)</span>
</div>
<p>One-step-ahead predictions (innovations) are often shown for MARSS models and these are used for likelihood calculations. Innovations are the difference between the data at time <span class="math inline">\(t\)</span> minus the prediction of <span class="math inline">\(\yy_t\)</span> given data up to <span class="math inline">\(t-1\)</span>. This section gives the residual variance for the innovations and the analogous values for the states. Innovations residuals are the more common residuals discussed for state-space models; these are also known as the one-step-ahead prediction residuals.</p>
<h2 id="one-step-ahead-model-residuals">One-step-ahead model residuals</h2>
<p>Define the innovations <span class="math inline">\(\checkvt\)</span> as: <span class="math display">\[\begin{equation}\label{eq:vtt1}
\checkvt = E[\YY_t|\yy^{(1)}_t] - \ZZ_t\hatxttm - \aa_t,
\end{equation}\]</span> where <span class="math inline">\(\hatxttm\)</span> is <span class="math inline">\(\E[\XX_t|\yy^{(1),t-1}]\)</span> (expected value of <span class="math inline">\(\XX_t\)</span> conditioned on the data up to time <span class="math inline">\(t-1\)</span>). The random variable, innovations over all possible <span class="math inline">\(\yy_t\)</span>, is <span class="math inline">\(\checkVt\)</span>. Its mean is 0 and we want to find its variance.</p>
<p>This is conceptually different than the observed innovations. First, this is the random variable innovation. <span class="math inline">\(\yy^{(1)}_t\)</span> here is not the actual data that you observe (the one data set that you have). It’s the data that you could observe. <span class="math inline">\(\yy_t\)</span> is a sample from the random variable <span class="math inline">\(\YY_t\)</span> and <span class="math inline">\(\checkvt\)</span> is a sample from the innovations you could observe. Second, <span class="math inline">\(\checkvt\)</span> includes both <span class="math inline">\(\yy_t^{(1)}\)</span> and <span class="math inline">\(\yy_t^{(2)}\)</span> (observed and unobserved <span class="math inline">\(\yy\)</span>). Normally, the innovations for missing data would appear as 0s, e.g., from a call to `MARSSfss()@. For missing data, <span class="math inline">\(\checkvt\)</span> is not necessarily 0. For example if <span class="math inline">\(\yy\)</span> is multivariate and correlated with each other through <span class="math inline">\(\RR\)</span> or a shared <span class="math inline">\(\xx\)</span> dependency.</p>
<p>The derivation of the variance of <span class="math inline">\(\checkVt\)</span> follows the exact same steps as the smoothations <span class="math inline">\(\hatVt\)</span>, except that we condition on the data up to <span class="math inline">\(t-1\)</span> not up to <span class="math inline">\(T\)</span>. Thus using Equation <span class="math inline">\(\ref{eq:first.and.secons.vvtgeneral}\)</span>, we can write the variance directly as: <span class="math display">\[\begin{equation}\label{eq:innov.model}
\var[\checkVt] = \RR_t - \ZZ_t \hatVttm \ZZ_t^\top + \hatSttm\ZZ_t^\top + \ZZ_t(\hatSttm)^\top
\end{equation}\]</span> where the <span class="math inline">\(\hatVttm\)</span> and <span class="math inline">\(\hatSttm\)</span> are now conditioned on only the data from 1 to <span class="math inline">\(t-1\)</span>. <span class="math inline">\(\hatSttm = \cov[\YY_t,\XX_t|\yy^{(1), t-1}] = \cov[\ZZ_t \XX_t + \aa_t+\VV_t,\XX_t|\yy^{(1), t-1}]\)</span>. <span class="math inline">\(\yy_t\)</span> is not in the conditional since it only includes data up to <span class="math inline">\(t-1\)</span>. Without <span class="math inline">\(\yy_t\)</span> in the conditional, <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\WW_t\)</span> and by extension <span class="math inline">\(\VV_t\)</span> and <span class="math inline">\(\XX_t\)</span> are independent and <span class="math inline">\(\cov[\ZZ_t \XX_t + \aa_t + \VV_t,\XX_t|\yy^{(1), t-1}] = \cov[\ZZ_t \XX_t,\XX_t|\yy^{(1), t-1}] = \ZZ_t \hatVttm\)</span>. Therefore, <span class="math inline">\(\ZZ_t(\hatSttm)^\top =  \ZZ_t \hatVttm \ZZ_t^\top = \hatSttm(\ZZ_t)^\top\)</span>. Thus Equation <span class="math inline">\(\ref{eq:innov.model}\)</span> reduces to <span class="math display">\[\begin{equation}\label{eq:innov.model2}
\var[\checkVt] = \RR_t + \ZZ_t \hatVttm \ZZ_t^\top.
\end{equation}\]</span></p>
<p>Note <span class="math inline">\(\var[\checkVt]\)</span> is a standard output from Kalman filter functions and is used to compute the likelihood of the data (conditioned on a set of parameters). In the Kalman filter in the {MARSS} package, it is output as  from .</p>
<h2 id="one-step-ahead-state-residuals">One-step ahead state residuals</h2>
<p>Define the state residuals conditioned on the data from 1 to <span class="math inline">\(t-1\)</span> as <span class="math inline">\(\checkwt\)</span>. <span class="math display">\[\begin{equation}\label{eq:wtt1}
\checkwt = \hatxtt - \BB_t\hatxtmt1 - \uu_t,
\end{equation}\]</span> where <span class="math inline">\(\hatxtmt1\)</span> is <span class="math inline">\(\E[\XX_{t-1}|\yy^{(1),t-1}]\)</span> (expected value of <span class="math inline">\(\XX_{t-1}\)</span> conditioned on the data up to time <span class="math inline">\(t-1\)</span>) and <span class="math inline">\(\hatxtt\)</span> is <span class="math inline">\(\E[\XX_{t}|\yy^{(1),t}]\)</span> (expected value of <span class="math inline">\(\XX_{t}\)</span> conditioned on the data up to time <span class="math inline">\(t\)</span>). From the Kalman filter equations: <span class="math display">\[\begin{equation}\label{eq:wtt1.2}
\hatxtt = \BB_t\hatxtmt1 + \uu_t + \KK_t \checkvt
\end{equation}\]</span> Thus, <span class="math inline">\(\checkwt\)</span> is a transformed <span class="math inline">\(\checkvt\)</span>: <span class="math display">\[\begin{equation}\label{eq:wtt1.3}
\checkwt = \KK_t \checkvt,
\end{equation}\]</span> where <span class="math inline">\(\KK_t\)</span> is the Kalman gain. <span class="math inline">\(\KK_t = \hatVttm \ZZ_t^\top[\ZZ_t \hatVttm \ZZ_t^\top + \RR_t]^{-1}\)</span> and <span class="math inline">\(\ZZ_t\)</span> is the missing values modified <span class="math inline">\(\ZZ_t\)</span> where if the <span class="math inline">\(i\)</span>-th <span class="math inline">\(\yy_t\)</span> is missing, the <span class="math inline">\(i\)</span>-th row of <span class="math inline">\(\ZZ_t\)</span> is set to all 0s (Shumway and Stoffer 2006, equation 6.78). Thus the variance of <span class="math inline">\(\checkWt\)</span> is <span class="math display">\[\begin{equation}\label{eq:var.Wt}
\var[\checkWt] = \KK_t \var[\checkVt] \KK_t^\top
\end{equation}\]</span></p>
<h2 id="joint-distribution-of-the-conditional-one-step-ahead-residuals">Joint distribution of the conditional one-step-ahead residuals</h2>
<h3 id="with-the-state-residuals-defined-from-t-1-to-t">with the state residuals defined from <span class="math inline">\(t-1\)</span> to <span class="math inline">\(t\)</span></h3>
<p>Define the one-step ahead residuals as <span class="math display">\[\begin{equation}
\overline{\varepsilon}_t = \begin{bmatrix}\checkvt\\ \checkwt \end{bmatrix}
\end{equation}\]</span></p>
<p>The covariance of <span class="math inline">\(\checkVt\)</span> and <span class="math inline">\(\checkWt\)</span> is <span class="math display">\[\begin{equation}\label{eq:covhatVtWtp.onestep}
\cov[\checkVt,\checkWtp] = \cov[\checkVt,\KK_{t}\checkVt] = \var[\checkVt]\KK_{t}^\top
\end{equation}\]</span></p>
<p>The joint variance-covariance matrix is <span class="math display">\[\begin{equation}\label{eq:jointcondresid.onestep}
\overline{\Sigma}_t = \var[\overline{\varepsilon}_t] = \begin{bmatrix}[c|c]
\var[\checkVt]&amp;  \var[\checkVt]\KK_{t}^\top\\
\rule[.5ex]{20ex}{0.25pt} &amp; \rule[.5ex]{20ex}{0.25pt} \\
\KK_{t}\var[\checkVt]&amp; \KK_{t}\var[\checkVt]\KK_{t}^\top \end{bmatrix},
\end{equation}\]</span></p>
<p>Since <span class="math inline">\(\checkwt=\KK_t \checkvt\)</span>, the state one-step-ahead residuals are perfectly correlated with the model one-step-ahead residuals so the joint distribution is not useful (all the information is in the variance of <span class="math inline">\(\checkVt\)</span>).</p>
<p>The Cholesky standardized residuals for <span class="math inline">\(\overline{\varepsilon}_t\)</span> are <span class="math display">\[\begin{equation}
\overline{\Sigma}_t^{-1/2} \overline{\varepsilon}_t
\end{equation}\]</span> However the Cholesky standardized joint residuals cannot be computed since <span class="math inline">\(\overline{\Sigma}_t\)</span> is not positive definite. Because <span class="math inline">\(\checkwt\)</span> equals <span class="math inline">\(\KK_t \checkvt\)</span>, the state residuals are completely explained by the model residuals. However we can compute the Cholesky standardized model residuals using <span class="math display">\[\begin{equation}
\var[\checkVt]^{-1/2} \checkvt
\end{equation}\]</span> <span class="math inline">\(\var[\checkVt]^{-1/2}\)</span> is the inverse of the lower triangle of the Cholesky decomposition of <span class="math inline">\(\var[\checkVt]\)</span>.</p>
<p>The marginal standardized joint residuals for <span class="math inline">\(\overline{\varepsilon}_t\)</span> (both model and state one-step ahead residuals) can be computed with the inverse of the square root of the diagonal of <span class="math inline">\(\overline{\Sigma}_t\)</span>: <span class="math display">\[\begin{equation}
\dg(\overline{\Sigma}_t)^{-1/2} \overline{\varepsilon}_t
\end{equation}\]</span> where <span class="math inline">\(dg(\AA)\)</span> is a diagonal matrix formed from the diagonal of the square matrix <span class="math inline">\(\AA\)</span>.</p>
<h3 id="with-the-state-residuals-defined-from-t-to-t1">with the state residuals defined from <span class="math inline">\(t\)</span> to <span class="math inline">\(t+1\)</span></h3>
<p></p>
<p>Define the one-step ahead residuals as <span class="math display">\[\begin{equation}
\overline{\varepsilon}_t^* = \begin{bmatrix}\checkvt\\ \checkwtp \end{bmatrix}
\end{equation}\]</span></p>
<p>The covariance of <span class="math inline">\(\checkVt\)</span> and <span class="math inline">\(\checkWtp\)</span> is <span class="math display">\[\begin{equation}\label{eq:covhatVtWtp.onestep.2}
\cov[\checkVt,\checkWtp] = \cov[\checkVt,\KK_{t+1}\checkVtp] = \cov[\checkVt, \checkVtp]\KK_{t+1}^\top
\end{equation}\]</span> Innovations residuals are temporally uncorrelated, thus <span class="math inline">\(\cov[\checkVt,\checkVtp]=0\)</span> and thus <span class="math inline">\(\cov[\checkVt,\checkWtp]=0\)</span>.</p>
<p>The joint variance-covariance matrix is <span class="math display">\[\begin{equation}\label{eq:jointcondresid.onestep2}
\overline{\Sigma}_t^* = \var[\overline{\varepsilon}_t^*] = \begin{bmatrix}[c|c]
\var[\checkVt]&amp;  0\\
\rule[.5ex]{20ex}{0.25pt} &amp; \rule[.5ex]{20ex}{0.25pt} \\
0 &amp; \KK_{t+1}\var[\checkVtp]\KK_{t+1}^\top \end{bmatrix},
\end{equation}\]</span></p>
<p>The Cholesky standardized residuals for <span class="math inline">\(\overline{\varepsilon}_t^*\)</span> are <span class="math display">\[\begin{equation}
(\overline{\Sigma}_t^*)^{-1/2} \overline{\varepsilon}_t^*
\end{equation}\]</span> <span class="math inline">\(\overline{\Sigma}_t^*\)</span> is positive definite so its Cholesky decomposition can be computed.</p>
<p>The marginal standardized joint residuals for <span class="math inline">\(\overline{\varepsilon}_t^*\)</span> are: <span class="math display">\[\begin{equation}
\dg(\overline{\Sigma}_t^*)^{-1/2} \overline{\varepsilon}_t^*
\end{equation}\]</span> where <span class="math inline">\(dg(\AA)\)</span> is a diagonal matrix formed from the diagonal of the square matrix <span class="math inline">\(\AA\)</span>.</p>
<h1 id="distribution-of-the-marss-contemporaneous-model-residuals">Distribution of the MARSS contemporaneous model residuals</h1>
<div style="display:none">
<span class="math inline">\(\nextSection\)</span>
</div>
<p>Contemporaneous model residuals are the difference between the data at time <span class="math inline">\(t\)</span> minus the prediction of <span class="math inline">\(\yy_t\)</span> given data up to <span class="math inline">\(t\)</span>. This section gives the residual variance for these residuals. There are no state residuals for this case as that would require the expected value of <span class="math inline">\(\XX_t\)</span> conditioned on the data up to <span class="math inline">\(t+1\)</span>.</p>
<p>Define the contemporaneous model residuals <span class="math inline">\(\dotvt\)</span> as: <span class="math display">\[\begin{equation}\label{eq:vtt}
\dotvt = E[\YY_t|\yy^{(1)}_t] - \ZZ_t\hatxtt - \aa_t,
\end{equation}\]</span> where <span class="math inline">\(\hatxtt\)</span> is <span class="math inline">\(\E[\XX_t|\yy^{(1),t}]\)</span> (expected value of <span class="math inline">\(\XX_t\)</span> conditioned on the data up to time <span class="math inline">\(t\)</span>). The random variable, contemporaneous model residuals over all possible <span class="math inline">\(\yy_t\)</span>, is <span class="math inline">\(\dotVt\)</span>. Its mean is 0 and we want to find its variance.</p>
<p>The derivation of the variance of <span class="math inline">\(\dotVt\)</span> follows the exact same steps as the smoothations <span class="math inline">\(\hatVt\)</span>, except that we condition on the data up to <span class="math inline">\(t\)</span> not up to <span class="math inline">\(T\)</span>. Thus using Equation <span class="math inline">\(\ref{eq:first.and.secons.vvtgeneral}\)</span>, we can write the variance directly as: <span class="math display">\[\begin{equation}\label{eq:contemp.model}
\var[\dotVt] = \RR_t - \ZZ_t \hatVtt \ZZ_t^\top + \hatStt\ZZ_t^\top + \ZZ_t(\hatStt)^\top
\end{equation}\]</span> where the <span class="math inline">\(\hatVtt\)</span> and <span class="math inline">\(\hatStt\)</span> are now conditioned on only the data from 1 to <span class="math inline">\(t\)</span>. <span class="math inline">\(\hatStt = \cov[\YY_t,\XX_t|\yy^{(1), t}] = \cov[\ZZ_t \XX_t + \aa_t+\VV_t,\XX_t|\yy^{(1), t}]\)</span>. If <span class="math inline">\(\yy_t\)</span> has no missing values, this reduces to <span class="math inline">\(\RR_t - \ZZ_t \hatVtt \ZZ_t^\top\)</span> while if <span class="math inline">\(\yy_t\)</span> is all missing values, this reduces to <span class="math inline">\(\RR_t + \ZZ_t \hatVtt \ZZ_t^\top\)</span>. See discussion of this after Equation <span class="math inline">\(\ref{eq:first.and.secons.vvtgeneral}\)</span>.</p>
<p>Equation <span class="math inline">\(\ref{eq:contemp.model}\)</span> gives the equation for the case where <span class="math inline">\(\yy_t\)</span> is partially observed. <span class="math inline">\(\hatStt\)</span> is output by the <code>MARSShatyt()</code> function in the {MARSS} package and <span class="math inline">\(\hatVtt\)</span> is output by the <code>MARSSkfss()</code> function.</p>
<h1 class="unnumbered" id="references">References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-CommandeurKoopman2007" class="csl-entry" role="listitem">
Commandeur, J. J. F., &amp; Koopman, S. J. (2007). <em>An introduction to state space time series analysis</em>. Oxford University Press.
</div>
<div id="ref-HarveyKoopman1992" class="csl-entry" role="listitem">
Harvey, A., &amp; Koopman, S. J. (1992). Diagnostic checking of unobserved-components time series models. <em>Journal of Business and Economic Statistics,</em> <em>10</em>, 377–389.
</div>
<div id="ref-Harveyetal1998" class="csl-entry" role="listitem">
Harvey, A., Koopman, S. J., &amp; Penzer, J. (1998). Messy time series: A unified approach. <em>Advances in Econometrics</em>, <em>13</em>, 103–143.
</div>
<div id="ref-Holmes2010" class="csl-entry" role="listitem">
Holmes, E. E. (2012). <em>Derivation of the <span>EM</span> algorithm for constrained and unconstrained <span>MARSS</span> models</em> [Technical report]. arXiv:1302.3919 [stat.ME].
</div>
<div id="ref-deJong1988" class="csl-entry" role="listitem">
Jong, P. de. (1988). A cross-validation filter for time series models. <em>Biometrika</em>, <em>75</em>(3), 594–600.
</div>
<div id="ref-deJongPenzer1998" class="csl-entry" role="listitem">
Jong, P. de, &amp; Penzer, J. (1998). Diagnosing shocks in time series. <em>Journal of the American Statistical Association</em>, <em>93</em>(442), 796–806.
</div>
<div id="ref-ShumwayStoffer2006" class="csl-entry" role="listitem">
Shumway, R. H., &amp; Stoffer, D. S. (2006). <em>Time series analysis and its applications</em> (2nd ed.). Springer-Science+Business Media, LLC.
</div>
</div>
